{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a022b01",
   "metadata": {},
   "source": [
    "# Day 10 - Syntax Scoring\n",
    "\n",
    ">You ask the submarine to determine the best route out of the deep-sea cave, but it only replies:\n",
    ">\n",
    ">Syntax error in navigation subsystem on line: all of them\n",
    "All of them?! The damage is worse than you thought. You bring up a copy of the navigation subsystem (your puzzle input).\n",
    ">\n",
    ">The navigation subsystem syntax is made of several lines containing chunks. There are one or more chunks on each line, and chunks contain zero or more other chunks. Adjacent chunks are not separated by any delimiter; if one chunk stops, the next chunk (if any) can immediately start. Every chunk must open and close with one of four legal pairs of matching characters:\n",
    ">\n",
    ">- If a chunk opens with `(`, it must close with `)`.\n",
    ">- If a chunk opens with `[`, it must close with `]`.\n",
    ">- If a chunk opens with `{`, it must close with `}`.\n",
    ">- If a chunk opens with `<`, it must close with `>`.\n",
    ">\n",
    ">So, `()` is a legal chunk that contains no other chunks, as is `[]`. More complex but valid chunks include `([])`, `{()()()}`, `<([{}])>`, `[<>({}){}[([])<>]]`, and even `(((((((((())))))))))`.\n",
    ">\n",
    ">Some lines are incomplete, but others are corrupted. Find and discard the corrupted lines first.\n",
    ">\n",
    ">A corrupted line is one where a chunk closes with the wrong character - that is, where the characters it opens and closes with do not form one of the four legal pairs listed above.\n",
    ">\n",
    ">Examples of corrupted chunks include `(]`, `{()()()>`, `(((()))}`, and `<([]){()}[{}])`. Such a chunk can appear anywhere within a line, and its presence causes the whole line to be considered corrupted.\n",
    ">\n",
    ">For example, consider the following navigation subsystem:\n",
    ">\n",
    ">```\n",
    "[({(<(())[]>[[{[]{<()<>>\n",
    "[(()[<>])]({[<{<<[]>>(\n",
    "{([(<{}[<>[]}>{[]{[(<()>\n",
    "(((({<>}<{<{<>}{[]{[]{}\n",
    "[[<[([]))<([[{}[[()]]]\n",
    "[{[{({}]{}}([{[{{{}}([]\n",
    "{<[[]]>}<{[{[{[]{()[[[]\n",
    "[<(<(<(<{}))><([]([]()\n",
    "<{([([[(<>()){}]>(<<{{\n",
    "<{([{{}}[<[[[<>{}]]]>[]]\n",
    ">```\n",
    ">\n",
    ">Some of the lines aren't corrupted, just incomplete; you can ignore these lines for now. The remaining five lines are corrupted:\n",
    ">\n",
    ">- `{([(<{}[<>[]}>{[]{[(<()>` - Expected ], but found } instead.\n",
    ">- `[[<[([]))<([[{}[[()]]]` - Expected ], but found ) instead.\n",
    ">- `[{[{({}]{}}([{[{{{}}([]` - Expected ), but found ] instead.\n",
    ">- `[<(<(<(<{}))><([]([]()` - Expected >, but found ) instead.\n",
    ">- `<{([([[(<>()){}]>(<<{{` - Expected ], but found > instead.\n",
    ">\n",
    ">Stop at the first incorrect closing character on each corrupted line.\n",
    ">\n",
    ">Did you know that syntax checkers actually have contests to see who can get the high score for syntax errors in a file? It's true! To calculate the syntax error score for a line, take the first illegal character on the line and look it up in the following table:\n",
    ">\n",
    ">- `)`: 3 points.\n",
    ">- `]`: 57 points.\n",
    ">- `}`: 1197 points.\n",
    ">- `>`: 25137 points.\n",
    ">\n",
    ">In the above example, an illegal ) was found twice (2*3 = 6 points), an illegal ] was found once (57 points), an illegal } was found once (1197 points), and an illegal > was found once (25137 points). So, the total syntax error score for this file is 6+57+1197+25137 = 26397 points!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c208da1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_input\n",
    "\n",
    "navigation = read_input(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e02ea2c",
   "metadata": {},
   "source": [
    "In this puzzle, we get to use [deque](https://docs.python.org/3/library/collections.html#collections.deque)! It's a data structure that's effective for last-in, first-out (LIFO) where we'll add and remove items to/from the end.\n",
    "\n",
    "To find an unmatching pair, we keep adding all the opening brackets into a `deque`, then every time we run into a closing bracket, we check if the last added opener is a match and if it is, we remove it and continue. Once we find a closer that is not matched with a corresponding opener, we stop, note it down as an error and move on to the next line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6f4e0efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "\n",
    "def check_errors(navigation):\n",
    "    pairs = { \n",
    "        '(': ')', \n",
    "        '[': ']', \n",
    "        '{': '}',\n",
    "        '<': '>'\n",
    "    }\n",
    "    \n",
    "    openers = deque()\n",
    "    errors = []\n",
    "    for line in navigation:\n",
    "        for char in line:\n",
    "            if char in pairs.keys():\n",
    "                openers.append(char)\n",
    "            else:\n",
    "                opener = openers.pop()\n",
    "                if char != pairs[opener]:\n",
    "                    errors.append(char)\n",
    "                    break\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3909f2e6",
   "metadata": {},
   "source": [
    "Scoring is quite straight-forward: we have a scoreboard for each mistaken closer and we sum over those points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6aff407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_errors(errors):\n",
    "    scoreboard = {\n",
    "        ')': 3,\n",
    "        ']': 57,\n",
    "        '}': 1197,\n",
    "        '>': 25137\n",
    "    }\n",
    "    \n",
    "    score = 0\n",
    "    for error in errors:\n",
    "        score += scoreboard[error]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e49420",
   "metadata": {},
   "source": [
    ">Find the first illegal character in each corrupted line of the navigation subsystem. **What is the total syntax error score for those errors?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6b218bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: 345441\n"
     ]
    }
   ],
   "source": [
    "found_errors = check_errors(navigation)\n",
    "error_score = score_errors(found_errors)\n",
    "print(f'Solution: {error_score}')\n",
    "assert error_score == 345441"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816291e4",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    ">Now, discard the corrupted lines. The remaining lines are incomplete.\n",
    ">\n",
    ">Incomplete lines don't have any incorrect characters - instead, they're missing some closing characters at the end of the line. To repair the navigation subsystem, you just need to figure out the sequence of closing characters that complete all open chunks in the line.\n",
    ">\n",
    ">You can only use closing characters (), ], }, or >), and you must add them in the correct order so that only legal pairs are formed and all chunks end up closed.\n",
    ">\n",
    ">In the example above, there are five incomplete lines:\n",
    ">\n",
    ">- `[({(<(())[]>[[{[]{<()<>>` - Complete by adding `}}]])})]`.\n",
    ">- `[(()[<>])]({[<{<<[]>>(` - Complete by adding `)}>]})`.\n",
    ">- `(((({<>}<{<{<>}{[]{[]{}` - Complete by adding `}}>}>))))`.\n",
    ">- `{<[[]]>}<{[{[{[]{()[[[]` - Complete by adding `]]}}]}]}>`.\n",
    ">- `<{([{{}}[<[[[<>{}]]]>[]]` - Complete by adding `])}>`.\n",
    ">\n",
    ">Did you know that autocomplete tools also have contests? It's true! The score is determined by considering the completion string character-by-character. Start with a total score of 0. Then, for each character, multiply the total score by 5 and then increase the total score by the point value given for the character in the following table:\n",
    ">\n",
    ">- `)`: 1 point.\n",
    ">- `]`: 2 points.\n",
    ">- `}`: 3 points.\n",
    ">- `>`: 4 points.\n",
    ">\n",
    ">So, the last completion string above - `])}>` - would be scored as follows:\n",
    ">\n",
    ">- Start with a total score of 0.\n",
    ">- Multiply the total score by 5 to get 0, then add the value of ] (2) to get a new total score of 2.\n",
    ">- Multiply the total score by 5 to get 10, then add the value of ) (1) to get a new total score of 11.\n",
    ">- Multiply the total score by 5 to get 55, then add the value of } (3) to get a new total score of 58.\n",
    ">- Multiply the total score by 5 to get 290, then add the value of > (4) to get a new total score of 294.\n",
    ">\n",
    ">The five lines' completion strings have total scores as follows:\n",
    ">\n",
    ">- `}}]])})]` - 288957 total points.\n",
    ">- `)}>]})` - 5566 total points.\n",
    ">- `}}>}>))))` - 1480781 total points.\n",
    ">- `]]}}]}]}>` - 995444 total points.\n",
    ">- `])}>` - 294 total points.\n",
    ">\n",
    ">Autocomplete tools are an odd bunch: the winner is found by sorting all of the scores and then taking the middle score. (There will always be an odd number of scores to consider.) In this example, the middle score is 288957 because there are the same number of scores smaller and larger than it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dbbe7c",
   "metadata": {},
   "source": [
    "After reading this second part, I was tempted to go and refactor the original function but decided for the sake of preserving my thought process to keep it and rewrite the function to only return the other side.\n",
    "\n",
    "If this was production code, I would have done both error finding and incomplete lines filtering in the same function so avoid double-looping. This is the challenge with Advent of Code puzzles sometimes: they are single-run and sometimes the first part's solution can become a bit confusing if mixed with the second part's code. I do it occasionally but today decided not to.\n",
    "\n",
    "_(See Appendix A below for this solution)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00cf250",
   "metadata": {},
   "source": [
    "This first function is essentially the same as in part 1 but instead of capturing the corrupted lines with mismatching brackets, we capture and collect the ones that did not error out.\n",
    "\n",
    "This was probably the first time I've ever used Python's `for..else` structure which is a bit of an odd ball. [for/else](https://book.pythontips.com/en/latest/for_-_else.html) works in a way that it is a regular `for` loop to start with but the `else` clause is executed only if the `for` loop \"ended naturally\", meaning there was no `break` used.\n",
    "\n",
    "I use it here to find if we did encounter a corrupt line (in which case we `break` out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8cf21355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "\n",
    "def analyze_navigation(navigation):\n",
    "    pairs = { \n",
    "        '(': ')', \n",
    "        '[': ']', \n",
    "        '{': '}',\n",
    "        '<': '>'\n",
    "    }\n",
    "    \n",
    "    openers = deque()\n",
    "    incomplete_lines = []\n",
    "\n",
    "    for line in navigation:\n",
    "        for char in line:\n",
    "            if char in pairs.keys():\n",
    "                openers.append(char)\n",
    "            else:\n",
    "                opener = openers.pop()\n",
    "                if char != pairs[opener]:\n",
    "                    break\n",
    "        else:\n",
    "            incomplete_lines.append(line)\n",
    "    return incomplete_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae0bbab",
   "metadata": {},
   "source": [
    "To find a solution to an incomplete line, I keep track of unmatched openers. \n",
    "\n",
    "If we encounter a new opener, we add it to the list.\n",
    "If we encounter a potential closer, we remove the last matching opener from the list.\n",
    "\n",
    "At the end, we have a list of unmatched openers so we find corresponding closers and then reverse that to get the final sequence.\n",
    "\n",
    "To remove the last matching opener, I reverse the list twice which is quite an expensive operation and there's probably a smarter way to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7ec76dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_solution(line):\n",
    "    open_to_closed = { \n",
    "        '(': ')', \n",
    "        '[': ']', \n",
    "        '{': '}',\n",
    "        '<': '>'\n",
    "    }\n",
    "    closed_to_open = {\n",
    "        ')': '(',\n",
    "        ']': '[',\n",
    "        '}': '{',\n",
    "        '>': '<'\n",
    "    }\n",
    "        \n",
    "    unmatched = []\n",
    "    for char in line:\n",
    "        if char in closed_to_open.values():\n",
    "            unmatched.append(char)\n",
    "        else:\n",
    "            unmatched.reverse()\n",
    "            unmatched.remove(closed_to_open[char])\n",
    "            unmatched.reverse()\n",
    "        \n",
    "    return ''.join([open_to_closed[opener] for opener in unmatched][::-1])\n",
    "\n",
    "\n",
    "def find_solutions(incomplete_lines):\n",
    "    return [\n",
    "        find_solution(line) for line in incomplete_lines\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef19169",
   "metadata": {},
   "source": [
    "Nothing special happening in scoring either. For each character, we multiple the score by 5 and add the corresponding score value for the closing bracket.\n",
    "\n",
    "We then sort the scores and get the middle value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8ad9cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_solutions(solutions):\n",
    "    scoreboard = {\n",
    "        ')': 1,\n",
    "        ']': 2,\n",
    "        '}': 3,\n",
    "        '>': 4\n",
    "    }\n",
    "    \n",
    "    scores = []\n",
    "    for solution in solutions:\n",
    "        score = 0\n",
    "        for char in solution:\n",
    "            score *= 5\n",
    "            score += scoreboard[char]\n",
    "        scores.append(score)\n",
    "        \n",
    "    scores = sorted(scores)\n",
    "    return scores[len(scores) // 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3649215",
   "metadata": {},
   "source": [
    "> Find the completion string for each incomplete line, score the completion strings, and sort the scores. **What is the middle score?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ed4fcdc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: 3235371166\n"
     ]
    }
   ],
   "source": [
    "incomplete_lines = analyze_navigation(navigation)\n",
    "solutions = find_solutions(incomplete_lines)\n",
    "score = score_solutions(solutions)\n",
    "\n",
    "print(f'Solution: {score}')\n",
    "assert score == 3235371166 # For refactoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2371be",
   "metadata": {},
   "source": [
    "## Appendix A - Combined Solution\n",
    "\n",
    "I mentioned in part 2 that I kinda wanted to go back to part 1 and put both of them in the same function. So I did it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3fa8e6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution for part 1: 345441\n",
      "Solution for part 2: 3235371166\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "\n",
    "def full_analysis(navigation):\n",
    "    pairs = { \n",
    "        '(': ')', \n",
    "        '[': ']', \n",
    "        '{': '}',\n",
    "        '<': '>'\n",
    "    }\n",
    "    \n",
    "    openers = deque()\n",
    "    incomplete_lines = []\n",
    "    errors = []\n",
    "\n",
    "    for line in navigation:\n",
    "        for char in line:\n",
    "            if char in pairs.keys():\n",
    "                openers.append(char)\n",
    "            else:\n",
    "                opener = openers.pop()\n",
    "                if char != pairs[opener]:\n",
    "                    errors.append(char)\n",
    "                    break\n",
    "        else:\n",
    "            incomplete_lines.append(line)\n",
    "    return errors, incomplete_lines\n",
    "\n",
    "\n",
    "p1, p2 = full_analysis(navigation)\n",
    "p1_score = score_errors(p1)\n",
    "print(f'Solution for part 1: {p1_score}')\n",
    "\n",
    "p2_score = score_solutions(find_solutions(p2))\n",
    "print(f'Solution for part 2: {score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
